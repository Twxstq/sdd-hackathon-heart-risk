{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./data/train.csv\")\n",
    "data_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "colomn_to_drop = pd.read_csv(\"./data/tabledrop.csv\")\n",
    "# ctd = list(set(list(colomn_to_drop[\"Colonne2\"].values)))\n",
    "ctd = [\"FMONTH\", \"IMONTH\", \"IYEAR\", \"DISPCODE\", \"PVTRESD1\", \"RENTHOM1\", \"CERVSCRN\", \"CRVCLPAP\", \"USENOW3\", \"_MAM5023\", \"_HADCOLN\", \"_CRCREC2\", \"_RACEG22\", \"_LLCPWT\", \"ID\"]\n",
    "# ctd += list(set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def clean_and_fill_data(data_train : pd.DataFrame, data_test : pd.DataFrame, column_to_drop) -> Tuple[pd.DataFrame, pd.DataFrame] :\n",
    "    # ici on va drop les colonnes\n",
    "    \n",
    "    # Fin du dropage des colonnes\n",
    "    \n",
    "    indexs_to_fill_with_0 = []\n",
    "    \n",
    "    missing_values = data_train.isnull().sum()\n",
    "    missing_values_treshold = missing_values[missing_values >= 1]\n",
    "    \n",
    "    for index in missing_values_treshold.index :\n",
    "        possible_value = set(data_train[index].dropna())\n",
    "        condition_categoriel = (len(possible_value) <= 100) # si on a moins de 100 réponse disponible on considère que c'est catégorielle. Donc on remplace par 0 au vue des questions.\n",
    "        condition_categoriel = condition_categoriel or 999 in possible_value # Dans certaine question 999 c'est refused de répondre\n",
    "        condition_categoriel = condition_categoriel or 777 in possible_value\n",
    "        condition_categoriel = condition_categoriel or 999999 in possible_value\n",
    "        condition_categoriel = condition_categoriel or 99 in possible_value\n",
    "        \n",
    "        if condition_categoriel :\n",
    "            indexs_to_fill_with_0.append(index)\n",
    "\n",
    "    missing_values = data_test.isnull().sum()\n",
    "    missing_values_treshold = missing_values[missing_values >= 1]\n",
    "    print(\"_RACE1\" in missing_values_treshold.index)\n",
    "    for index in missing_values_treshold.index :\n",
    "        possible_value = set(data_test[index].dropna())\n",
    "        condition_categoriel = (len(possible_value) <= 100) # si on a moins de 100 réponse disponible on considère que c'est catégorielle. Donc on remplace par 0 au vue des questions.\n",
    "        condition_categoriel = condition_categoriel or 999 in possible_value # Dans certaine question 999 c'est refused de répondre\n",
    "        condition_categoriel = condition_categoriel or 777 in possible_value\n",
    "        condition_categoriel = condition_categoriel or 999999 in possible_value\n",
    "        condition_categoriel = condition_categoriel or 99 in possible_value\n",
    "\n",
    "        if condition_categoriel :\n",
    "            indexs_to_fill_with_0.append(index)\n",
    "        \n",
    "    for index in indexs_to_fill_with_0 :\n",
    "        data_train[index] = data_train[index].fillna(0)\n",
    "        data_test[index] = data_test[index].fillna(0)\n",
    "    \n",
    "    \n",
    "    indexs_to_fill_with_mean = [\"_CLLCPWT\", \"WTKG3\", \"_BMI5\"]\n",
    "    for index in indexs_to_fill_with_mean :\n",
    "        data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        \n",
    "        data_train[index] = data_train[index].fillna(data_train[index].dropna().mean())\n",
    "        data_test[index] = data_test[index].fillna(data_test[index].dropna().mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # On modifier la taille ici\n",
    "    data_train = data_train.drop(\"HEIGHT3\", axis=1)\n",
    "    data_test = data_test.drop(\"HEIGHT3\", axis=1)\n",
    "    \n",
    "    data_train[\"_PACKDAY\"] = data_train[\"LCSNUMCG\"]/20\n",
    "    data_test[\"_PACKDAY\"] = data_test[\"LCSNUMCG\"]/20\n",
    "    \n",
    "    data_train[\"_PACKYRS\"] = round(data_train[\"_YRSSMOK\"]*data_train[\"_PACKDAY\"])\n",
    "    data_test[\"_PACKYRS\"] = round(data_test[\"_YRSSMOK\"]*data_test[\"_PACKDAY\"])\n",
    "    \n",
    "\n",
    "    value_map = {\n",
    "    1: 10,\n",
    "    2: 5,\n",
    "    3: 0,\n",
    "    4: -5,\n",
    "    5: -10,\n",
    "    7: 0,\n",
    "    9: 0,\n",
    "    np.nan: 0  # Handle NaN explicitly\n",
    "    }\n",
    "    \n",
    "    data_train[\"GENHLTH\"] = data_train[\"GENHLTH\"].replace(value_map)\n",
    "    data_test[\"GENHLTH\"] = data_test[\"GENHLTH\"].replace(value_map) \n",
    "\n",
    "    value_map = {\n",
    "        88 : 0,\n",
    "        99: 0,\n",
    "        77 : 15,\n",
    "        np.nan: 0  # Handle NaN explicitly\n",
    "    }\n",
    "    data_train[\"PHYSHLTH\"] = data_train[\"PHYSHLTH\"].replace(value_map)\n",
    "    data_test[\"PHYSHLTH\"] = data_test[\"PHYSHLTH\"].replace(value_map) \n",
    "\n",
    "    value_map = {\n",
    "        88 : 0,\n",
    "        99: 0,\n",
    "        77 : 15,\n",
    "        np.nan: 0  # Handle NaN explicitly\n",
    "    }\n",
    "    data_train[\"MENTHLTH\"] = data_train[\"MENTHLTH\"].replace(value_map)\n",
    "    data_test[\"MENTHLTH\"] = data_test[\"MENTHLTH\"].replace(value_map)\n",
    "\n",
    "\n",
    "    # data_train[\"_BMI5\"] = (data_train[\"WTKG3\"]/(data_train[\"HTM4\"]*data_train[\"HTM4\"])).apply(lambda x : max(1,x))\n",
    "    # data_test[\"_BMI5\"] = (data_test[\"WTKG3\"]/(data_test[\"HTM4\"]*data_test[\"HTM4\"])).apply(lambda x : max(1,x))\n",
    "\n",
    "    \n",
    "    \n",
    "    for column in column_to_drop :\n",
    "        data_train = data_train.drop(column, axis=1)\n",
    "        data_test = data_test.drop(column, axis=1)\n",
    "        \n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "    \n",
    "data_train_clean, data_test_clean = clean_and_fill_data(data_train=data_train,\n",
    "                                                        data_test=data_test,\n",
    "                                                        column_to_drop=ctd)\n",
    "\n",
    "# data_train_clean.to_csv(\"./data/train1.csv\")\n",
    "# data_test_clean.to_csv(\"./data/test1.csv\")\n",
    "\n",
    "print(np.isinf(data_train_clean).sum().sum())\n",
    "print(np.isinf(data_test_clean).sum().sum())\n",
    "\n",
    "print(data_train_clean.isnull().sum().sum())\n",
    "print(data_test_clean.isnull().sum().sum())\n",
    "data_test_clean.isnull().sum()[data_test_clean.isnull().sum() != 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_test_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ID'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 225000 entries, 0 to 224999\n",
      "Columns: 311 entries, _STATE to TARGET\n",
      "dtypes: bool(1), float64(306), int64(4)\n",
      "memory usage: 532.4 MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = data_train_clean.iloc[:, :-1]  # All columns except the last one\n",
    "y = data_train_clean.iloc[:, -1]   # The last column is the target\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
