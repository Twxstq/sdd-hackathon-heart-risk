{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./data/train.csv\")\n",
    "data_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "colomn_to_drop = pd.read_csv(\"./data/tabledrop.csv\")\n",
    "# ctd = list(set(list(colomn_to_drop[\"Colonne2\"].values)))\n",
    "ctd = [\"FMONTH\", \"IMONTH\", \"IYEAR\", \"DISPCODE\", \"PVTRESD1\", \"RENTHOM1\", \"CERVSCRN\", \"CRVCLPAP\", \"USENOW3\", \"_MAM5023\", \"_HADCOLN\", \"_CRCREC2\", \"_RACEG22\", \"_LLCPWT\", \"ID\"]\n",
    "# ctd += list(set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def clean_and_fill_data(data_train : pd.DataFrame, data_test : pd.DataFrame, column_to_drop) -> Tuple[pd.DataFrame, pd.DataFrame] :\n",
    "    # ici on va drop les colonnes\n",
    "    \n",
    "    # Fin du dropage des colonnes\n",
    "    \n",
    "    indexs_to_fill_with_0 = []\n",
    "    \n",
    "    missing_values = data_train.isnull().sum()\n",
    "    missing_values_treshold = missing_values[missing_values >= 1]\n",
    "    \n",
    "    for index in missing_values_treshold.index :\n",
    "        possible_value = set(data_train[index].dropna())\n",
    "        condition_categoriel = (len(possible_value) <= 100) # si on a moins de 100 réponse disponible on considère que c'est catégorielle. Donc on remplace par 0 au vue des questions.\n",
    "        condition_categoriel = condition_categoriel or 999 in possible_value # Dans certaine question 999 c'est refused de répondre\n",
    "        condition_categoriel = condition_categoriel or 777 in possible_value\n",
    "        condition_categoriel = condition_categoriel or 999999 in possible_value\n",
    "        condition_categoriel = condition_categoriel or 99 in possible_value\n",
    "        \n",
    "        if condition_categoriel :\n",
    "            indexs_to_fill_with_0.append(index)\n",
    "\n",
    "    missing_values = data_test.isnull().sum()\n",
    "    missing_values_treshold = missing_values[missing_values >= 1]\n",
    "    print(\"_RACE1\" in missing_values_treshold.index)\n",
    "    for index in missing_values_treshold.index :\n",
    "        possible_value = set(data_test[index].dropna())\n",
    "        condition_categoriel = (len(possible_value) <= 100) # si on a moins de 100 réponse disponible on considère que c'est catégorielle. Donc on remplace par 0 au vue des questions.\n",
    "        condition_categoriel = condition_categoriel or 999 in possible_value # Dans certaine question 999 c'est refused de répondre\n",
    "        condition_categoriel = condition_categoriel or 777 in possible_value\n",
    "        condition_categoriel = condition_categoriel or 999999 in possible_value\n",
    "        condition_categoriel = condition_categoriel or 99 in possible_value\n",
    "\n",
    "        if condition_categoriel :\n",
    "            indexs_to_fill_with_0.append(index)\n",
    "        \n",
    "    for index in indexs_to_fill_with_0 :\n",
    "        data_train[index] = data_train[index].fillna(0)\n",
    "        data_test[index] = data_test[index].fillna(0)\n",
    "    \n",
    "    \n",
    "    indexs_to_fill_with_mean = [\"_CLLCPWT\", \"WTKG3\", \"_BMI5\"]\n",
    "    for index in indexs_to_fill_with_mean :\n",
    "        data_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        data_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        \n",
    "        data_train[index] = data_train[index].fillna(data_train[index].dropna().mean())\n",
    "        data_test[index] = data_test[index].fillna(data_test[index].dropna().mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # On modifier la taille ici\n",
    "    data_train = data_train.drop(\"HEIGHT3\", axis=1)\n",
    "    data_test = data_test.drop(\"HEIGHT3\", axis=1)\n",
    "    \n",
    "    data_train[\"_PACKDAY\"] = data_train[\"LCSNUMCG\"]/20\n",
    "    data_test[\"_PACKDAY\"] = data_test[\"LCSNUMCG\"]/20\n",
    "    \n",
    "    data_train[\"_PACKYRS\"] = round(data_train[\"_YRSSMOK\"]*data_train[\"_PACKDAY\"])\n",
    "    data_test[\"_PACKYRS\"] = round(data_test[\"_YRSSMOK\"]*data_test[\"_PACKDAY\"])\n",
    "    \n",
    "\n",
    "    value_map = {\n",
    "    1: 10,\n",
    "    2: 5,\n",
    "    3: 0,\n",
    "    4: -5,\n",
    "    5: -10,\n",
    "    7: 0,\n",
    "    9: 0,\n",
    "    np.nan: 0  # Handle NaN explicitly\n",
    "    }\n",
    "    \n",
    "    data_train[\"GENHLTH\"] = data_train[\"GENHLTH\"].replace(value_map)\n",
    "    data_test[\"GENHLTH\"] = data_test[\"GENHLTH\"].replace(value_map) \n",
    "\n",
    "    value_map = {\n",
    "        88 : 0,\n",
    "        99: 0,\n",
    "        77 : 15,\n",
    "        np.nan: 0  # Handle NaN explicitly\n",
    "    }\n",
    "    data_train[\"PHYSHLTH\"] = data_train[\"PHYSHLTH\"].replace(value_map)\n",
    "    data_test[\"PHYSHLTH\"] = data_test[\"PHYSHLTH\"].replace(value_map) \n",
    "\n",
    "    value_map = {\n",
    "        88 : 0,\n",
    "        99: 0,\n",
    "        77 : 15,\n",
    "        np.nan: 0  # Handle NaN explicitly\n",
    "    }\n",
    "    data_train[\"MENTHLTH\"] = data_train[\"MENTHLTH\"].replace(value_map)\n",
    "    data_test[\"MENTHLTH\"] = data_test[\"MENTHLTH\"].replace(value_map)\n",
    "\n",
    "\n",
    "    # data_train[\"_BMI5\"] = (data_train[\"WTKG3\"]/(data_train[\"HTM4\"]*data_train[\"HTM4\"])).apply(lambda x : max(1,x))\n",
    "    # data_test[\"_BMI5\"] = (data_test[\"WTKG3\"]/(data_test[\"HTM4\"]*data_test[\"HTM4\"])).apply(lambda x : max(1,x))\n",
    "\n",
    "    \n",
    "    \n",
    "    for column in column_to_drop :\n",
    "        data_train = data_train.drop(column, axis=1)\n",
    "        data_test = data_test.drop(column, axis=1)\n",
    "        \n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "    \n",
    "data_train_clean, data_test_clean = clean_and_fill_data(data_train=data_train,\n",
    "                                                        data_test=data_test,\n",
    "                                                        column_to_drop=ctd)\n",
    "\n",
    "# data_train_clean.to_csv(\"./data/train1.csv\")\n",
    "# data_test_clean.to_csv(\"./data/test1.csv\")\n",
    "\n",
    "print(np.isinf(data_train_clean).sum().sum())\n",
    "print(np.isinf(data_test_clean).sum().sum())\n",
    "\n",
    "print(data_train_clean.isnull().sum().sum())\n",
    "print(data_test_clean.isnull().sum().sum())\n",
    "data_test_clean.isnull().sum()[data_test_clean.isnull().sum() != 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
